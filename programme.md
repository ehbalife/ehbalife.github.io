---
title: Programme
layout: page

menu: true
order: 3
---

**Keynote speakers**

[**Gilbert Roberts**](https://www.researchgate.net/profile/Gilbert_Roberts)\\
**The role of reputation in the evolution of cooperation**\\
Cooperation is common in nature and is central to human success, but is difficult to explain when it involves individuals helping others at an apparent cost to themselves. Theories of kinship and of reciprocation are well established but cooperation seems to extend well beyond these bounds. Much of my research is on the role of reputation in promoting cooperation. An individual’s reputation in this context is an index of how cooperative it appears to be based on past behaviour. The core idea is that individuals benefit from being seen to be cooperative. Developing a ‘good’ reputation could bring benefits if it influences others to be more cooperative. The most popular theory of how individuals gain is ‘indirect reciprocity’. If X pays some cost (‘cooperates’) to benefit Y, then Z assigns X a good reputation and is more likely to help X in future, so X can make a net profit. I will present some results from agent-based simulations of indirect reciprocity and show how previous conclusions depend crucially on the effects of relatedness. I show how indirect reciprocity is unlikely to be a widespread mechanism accounting for reputation-building. I suggest instead that individuals who display cooperativeness benefit from being more likely to be chosen for social or sexual partnerships. This raises interesting questions about when past generosity is an honest signal of future behaviour, and when individuals should use reputations in choosing partners. I addressed these questions using agent based simulation models in which agents could optionally (1) invest in a cooperative reputation by helping others; (2) choose a partner with a generous reputation; and (3) cooperate in a repeated social dilemma game. I found that these independent traits became linked: reputation was an honest signal of cooperation when it was strategic for cooperators to invest in the benefits of reputation. I conclude that building a cooperative reputation by being generous can be an honest signal of trustworthiness which it pays to attend to when choosing a partner. I suggest that this kind of reputational signalling can help to explain phenomena including philanthropy, collective action, punishment, courtship and advertising.

[**Peter Lewis**](https://www2.aston.ac.uk/eas/staff/a-z/dr-peter-lewis)\\ 
**What it it like to trust a rock?**\\
We are entering into a world where many of the machines that we are
developing, using, and working with on a day-to-day basis behave in ways
that we neither understand nor can fully control. The complexity
associated with so-called intelligent systems, and how they work, is
fast surpassing human understanding, yet this complexity is often where
their value lies. It is perhaps not surprising, then, that ‘trust’ has
become a hot topic in AI, but what does it mean to trust a machine? Can
machines really be trustworthy? How should we make trust decisions,
given the nature of machines? And should this be the same as the way
humans make decisions when deciding whether to trust other people or
animals, or is it more like the informal way we might talk of trusting a
car to start on a cold morning? After Sloman, I will construct a thought
experiment: ‘what is it like to trust X’, where X may be any of the wide
variety of things animate, inanimate, human, animal, vegetable or
mineral, that are both found and built by people. Our intention is to
propose and illustrate the generality of an integrated model for trust
decisions, and further, to illustrate how instantiations of it vary
between kinds of objects that we might be considering trusting in a
particular context.

**Contributed talks** (10min presentation + 5 min questions)

More details coming soon
